{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d21f7bf-98ca-45d1-8800-618c833986c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the word 'dog', the most similar word is 'cat' with cosine similarity = 0.9218\n",
      "For the word 'whale', the most similar word is 'whales' with cosine similarity = 0.8987\n",
      "For the word 'before', the most similar word is 'after' with cosine similarity = 0.9512\n",
      "For the word 'however', the most similar word is 'although' with cosine similarity = 0.9801\n",
      "For the word 'fabricate', the most similar word is 'fabricating' with cosine similarity = 0.7595\n"
     ]
    }
   ],
   "source": [
    "# Author: Hassan Ali\n",
    "# Problem 10.3: Using GloVe and Cosine Similarity\n",
    "# Objective: Find the most similar word for each from a given list.\n",
    "\n",
    "# Importing Libraries\n",
    "import torch\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Loading GloVe embeddings\n",
    "# \"6B\" = trained on Wikipedia 2014 + Gigaword 5 | dim=50 means each word vector is 50-dimensional.\n",
    "glove = GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "# Helper function to compute cosine similarity between two 1D tensors (embedding vectors).\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1, vec2, dim=0).item()\n",
    "\n",
    "# DFunction to find the most similar word to a given input word (excluding the word itself).\n",
    "def find_most_similar_word(word, glove_vectors):\n",
    "    # If the word is not in the vocabulary, return None.\n",
    "    if word not in glove_vectors.stoi:\n",
    "        return None, None\n",
    "\n",
    "    # Getting the vector for the input word\n",
    "    target_vec = glove_vectors[word]\n",
    "    best_sim = float(\"-inf\")\n",
    "    best_word = None\n",
    "\n",
    "    # Iterating through all vocabulary words in GloVe\n",
    "    for candidate in glove_vectors.stoi.keys():\n",
    "        # Skipping the same word\n",
    "        if candidate == word:\n",
    "            continue\n",
    "        # Computing similarity\n",
    "        candidate_vec = glove_vectors[candidate]\n",
    "        sim = cosine_similarity(target_vec, candidate_vec)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_word = candidate\n",
    "\n",
    "    return best_word, best_sim\n",
    "\n",
    "# Words to check\n",
    "words_to_check = [\"dog\", \"whale\", \"before\", \"however\", \"fabricate\"]\n",
    "\n",
    "# For each word, find the most similar word and print the result\n",
    "for w in words_to_check:\n",
    "    most_similar, similarity_score = find_most_similar_word(w, glove)\n",
    "    if most_similar is None:\n",
    "        print(f\"'{w}' is not in the GloVe vocabulary.\")\n",
    "    else:\n",
    "        print(f\"For the word '{w}', the most similar word is '{most_similar}' \"\n",
    "              f\"with cosine similarity = {similarity_score:.4f}\")\n",
    "\n",
    "\n",
    "# Analysis:\n",
    "# - Loading the GloVe vectors from the '6B' dataset with 50-dimensional embeddings. This\n",
    "#  dataset has been trained on a large text corpus (Wikipedia 2014 + Gigaword 5). Each\n",
    "#  word in the vocabulary is mapped to a vector of length 50, capturing semantic and\n",
    "#  syntactic information.\n",
    "#  - Cosine similarity measures how close two vectors are in terms of angle rather than\n",
    "#  magnitude. A value of 1.0 means the vectors point in exactly the same direction,\n",
    "#  while a value of -1.0 means they point in opposite directions. A value of 0.0 means\n",
    "#  they are orthogonal (unrelated). In this script, we compute similarity for every\n",
    "#  vocabulary word in GloVe and pick the one that yields the highest similarity.\n",
    "# - Iterating through the entire GloVe vocabulary for each query word, computing\n",
    "#  cosine similarity. Since GloVe has a large vocabulary, this can be time-consuming\n",
    "#  for many queries, but is straightforward for a small set of words. We skip the\n",
    "#  original word itself to avoid trivial matches.\n",
    "#  - Five chosen words: 'dog', 'whale', 'before', 'however', and 'fabricate'. \n",
    "#  For each, the code prints the word in the GloVe vocabulary that is most similar\n",
    "#  in meaning or usage. The reported cosine similarity indicates how close the two\n",
    "#  word vectors are. \n",
    "# - In an example run, 'dog' is closest to 'cat' with similarity around 0.92,\n",
    "#  'whale' is closest to 'whales' (approx. 0.90), 'before' is closest to 'after'\n",
    "#  (approx. 0.95), 'however' is closest to 'although' (approx. 0.98), and 'fabricate'\n",
    "#  is closest to 'fabricating' (approx. 0.76). These results are plausible given how\n",
    "#  GloVe embeddings capture semantic relationships.\n",
    "#  - Functionality depends on the presence of words in GloVeâ€™s vocabulary. Words that\n",
    "#   are not present will return None.\n",
    "#  - Cosine similarity in GloVe space often correlates with shared semantic or \n",
    "#   syntactic context. \n",
    "#  - High similarity typically indicates that two words appear in similar contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741f9304-e8a3-4147-9658-db9abc2b5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "Epoch 1/5 | Train Loss: 0.6221 | Train Acc: 74.53% | Valid Loss: 0.5064 | Valid Acc: 79.05%\n",
      "Epoch 2/5 | Train Loss: 0.4745 | Train Acc: 81.02% | Valid Loss: 0.4780 | Valid Acc: 81.68%\n",
      "Epoch 3/5 | Train Loss: 0.4457 | Train Acc: 81.98% | Valid Loss: 0.4282 | Valid Acc: 82.77%\n",
      "Epoch 4/5 | Train Loss: 0.4246 | Train Acc: 83.13% | Valid Loss: 0.4194 | Valid Acc: 82.95%\n",
      "Epoch 5/5 | Train Loss: 0.4109 | Train Acc: 83.49% | Valid Loss: 0.4222 | Valid Acc: 83.27%\n",
      "Test Loss: 0.4280 | Test Acc: 82.91%\n",
      "Tweet: I absolutely love this airline! Great service and comfortable seats. --> Predicted Sentiment: positive\n",
      "Tweet: The flight was delayed and the staff was extremely rude. --> Predicted Sentiment: negative\n",
      "Tweet: My experience was just okay; nothing special. --> Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Author: Hassan Ali\n",
    "# Problem 10.5 (BERT-based Tweet Sentiment Classification)\n",
    "# Dataset: Tweets.csv (US Airline Sentiment or a similar tweet dataset)\n",
    "# Building a BERT-based model augmented with a GRU layer for tweet sentiment classification.\n",
    "# The model will classify tweets into multiple sentiment classes (e.g., negative, neutral, positive).\n",
    "\n",
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Loading and preprocess the dataset\n",
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "# Renaming columns for clarity (assume original columns: 'airline_sentiment' and 'text')\n",
    "df = df.rename(columns={\"airline_sentiment\": \"label\", \"text\": \"tweet\"})\n",
    "# Dropping rows with missing tweet text or label\n",
    "df = df.dropna(subset=[\"label\", \"tweet\"]).copy()\n",
    "# Retaining only valid sentiment labels (adjust if needed)\n",
    "valid_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "df = df[df[\"label\"].isin(valid_labels)].copy()\n",
    "# Encodnig sentiment labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"encoded_label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "print(\"Label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "\n",
    "# Splitting data into training, validation, and test sets\n",
    "# Stratifying splits to preserve class distribution; 70% train, 15% validation, 15% test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=SEED, stratify=df[\"encoded_label\"])\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"encoded_label\"])\n",
    "\n",
    "\n",
    "# Defining a custom Dataset class for tweets\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Tokenizing the text with padding and truncation\n",
    "        tokens = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        # Removing batch dimension\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Initializing the BERT tokenizer (using bert-base-uncased)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Creating Dataset objects for each split\n",
    "train_dataset = TweetDataset(train_df[\"tweet\"], train_df[\"encoded_label\"], tokenizer)\n",
    "valid_dataset = TweetDataset(valid_df[\"tweet\"], valid_df[\"encoded_label\"], tokenizer)\n",
    "test_dataset  = TweetDataset(test_df[\"tweet\"], test_df[\"encoded_label\"], tokenizer)\n",
    "\n",
    "# Defining DataLoader batch size\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Defining the BERT-GRU model for tweet sentiment classification\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, num_classes, dropout):\n",
    "        super(BERTGRUSentiment, self).__init__()\n",
    "        self.bert = bert  # Pre-trained BERT model\n",
    "        # GRU layer: bidirectional; input size equals BERT's hidden size\n",
    "        self.gru = nn.GRU(bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # Fully connected layer: concatenated hidden states from both directions\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Obtaining embeddings from BERT (BERT parameters are frozen during training)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embedded = outputs.last_hidden_state\n",
    "        # Passing embeddings through GRU layer; hidden shape: [num_layers * num_directions, batch, hidden_dim]\n",
    "        _, hidden = self.gru(embedded)\n",
    "        # Concatenating the final forward and backward hidden states\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        hidden_cat = self.dropout(hidden_cat)\n",
    "        # Fully connected layer produces class logits\n",
    "        return self.fc(hidden_cat)\n",
    "\n",
    "# Loading the pre-trained BERT model\n",
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Setting number of sentiment classes (3: negative, neutral, positive)\n",
    "num_classes = len(valid_labels)\n",
    "# Initializing the BERT-GRU model with hidden dimension 128 and dropout rate 0.3\n",
    "model = BERTGRUSentiment(bert, hidden_dim=128, num_classes=num_classes, dropout=0.3)\n",
    "\n",
    "# Freezing BERT parameters to reduce training time\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Defining loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Define training and evaluation functions\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for input_ids, attention_mask, labels in loader:\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        acc = (predictions == labels).float().mean()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            acc = (predictions == labels).float().mean()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_epoch(model, valid_loader, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"bert_gru_tweet_model.pt\")\n",
    "    print(f\"Epoch {epoch}/{N_EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.load_state_dict(torch.load(\"bert_gru_tweet_model.pt\", weights_only=True))\n",
    "test_loss, test_acc = evaluate_epoch(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Inference on sample tweets\n",
    "def predict_sentiment(model, tokenizer, text):\n",
    "    model.eval()\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, pred_label = torch.max(outputs, dim=1)\n",
    "    return label_encoder.inverse_transform(pred_label.cpu().numpy())\n",
    "\n",
    "sample_texts = [\n",
    "    \"I absolutely love this airline! Great service and comfortable seats.\",\n",
    "    \"The flight was delayed and the staff was extremely rude.\",\n",
    "    \"My experience was just okay; nothing special.\"\n",
    "]\n",
    "for text in sample_texts:\n",
    "    sentiment = predict_sentiment(model, tokenizer, text)\n",
    "    print(f\"Tweet: {text} --> Predicted Sentiment: {sentiment[0]}\")\n",
    "\n",
    "\n",
    "# Analysis:\n",
    "# The script loads the Tweets.csv dataset, changes the names of the columns  to more meaningful names, excludes the rows that\n",
    "# have an empty tweet or label, and selects only those  tweets that have appropriate sentiment labels (negative, neutral, positive).\n",
    "# Sentiment labels are encoded  as integers and the data is divided into training, validation and test sets with stratification.\n",
    "# A PyTorch Dataset class is defined to tokenize the tweet texts using a BERT tokenizer which is  pretrained.\n",
    "# The model architecture consists of the following: To get embeddings of a sequence, a frozen  BERT model is used, a bidirectional\n",
    "# GRU layer is used to capture the sequential dependence,  and a fully connected layer which gives the class logits for multi class\n",
    "# sentiment classification.\n",
    "# The training loop  is trained for 5 epochs using the Adam optimizer and CrossEntropyLoss; the model that has the  best validation\n",
    "# loss is saved and then used to evaluate the performance of the model on the test set.\n",
    "# Last, the model does inference on some example tweets and for each tweets, it sets the sentiment label of the tweets.\n",
    "# The torch.load function is called with the weights only option in order to  avoid security issues.\n",
    "# This pipeline shows a typical way of training a BERT based model with additional  GRU layer for the task of sentiment analysis\n",
    "# on tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678b70d-f9b2-4ae0-9e70-64d2c5a87eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
